{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Data Preprocessing (PREP)](#toc0_)\n",
    "\n",
    "**Objective:** Clean and preprocess the data to ensure it is standardized, free of errors, and ready for integration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Data Preprocessing (PREP)](#toc1_)    \n",
    "  - [Parameters](#toc1_1_)    \n",
    "  - [Imports](#toc1_2_)    \n",
    "  - [Load input data](#toc1_3_)    \n",
    "  - [Counting unique values to identify classes](#toc1_4_)    \n",
    "  - [Check for deleted samples](#toc1_5_)    \n",
    "  - [Add `URL` colum to the data](#toc1_6_)    \n",
    "  - [Show number of samples by `sample_name`](#toc1_7_)    \n",
    "  - [Exclude samples with issues](#toc1_8_)    \n",
    "  - [Find and check samples with `sample_name`==`unknown` or empty](#toc1_9_)    \n",
    "  - [Checking if there are any missing and unknown values in the `processed_file_location` column](#toc1_10_)    \n",
    "- [Ckeck the urls for the `processed_file_location` column](#toc2_)    \n",
    "    - [Add colum n image_name if doesn't exist](#toc2_1_1_)    \n",
    "  - [Hash](#toc2_2_)    \n",
    "  - [Save Cleaned Data](#toc2_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'MHS-Tanzania'\n",
    "INPUT_DATA_FNAME = \"data/processed/recovered_data.csv\"\n",
    "OUTPUT_DATA_FNAME = \"data/processed/MSH-Tanzania_metadata_all.csv\"\n",
    "TMP_IMG_DIR = 'data/intermediate/images/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../src/\")\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Load input data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 2949\n",
      "Columns: 27\n",
      "\tIndex(['id', 'sample_name', 'test_name', 'user_name', 'date_of_creation',\n",
      "       'raw_file_location', 'processed_file_location', 'processing_date',\n",
      "       'camera_type_1', 'notes', 'sample_id', 'quantity', 'deleted', 'issue',\n",
      "       'project.id', 'project.user_name', 'project.project_name',\n",
      "       'project.annotation', 'project.test_name',\n",
      "       'project.sample_names.sample_names', 'project.neutral_filler',\n",
      "       'project.qpc20', 'project.qpc50', 'project.qpc80', 'project.qpc100',\n",
      "       'project.notes', 'test_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "initial_data = pd.read_csv(INPUT_DATA_FNAME)\n",
    "print(f\"Samples: {len(initial_data)}\")\n",
    "print(f\"Columns: {len(initial_data.columns)}\")\n",
    "print(f\"\\t{(initial_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2949 entries, 0 to 2948\n",
      "Data columns (total 27 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   id                                 2949 non-null   int64  \n",
      " 1   sample_name                        2949 non-null   object \n",
      " 2   test_name                          2949 non-null   object \n",
      " 3   user_name                          2949 non-null   object \n",
      " 4   date_of_creation                   2949 non-null   object \n",
      " 5   raw_file_location                  2944 non-null   object \n",
      " 6   processed_file_location            2949 non-null   object \n",
      " 7   processing_date                    2949 non-null   object \n",
      " 8   camera_type_1                      2949 non-null   object \n",
      " 9   notes                              2949 non-null   object \n",
      " 10  sample_id                          2949 non-null   int64  \n",
      " 11  quantity                           2949 non-null   int64  \n",
      " 12  deleted                            2949 non-null   bool   \n",
      " 13  issue                              0 non-null      float64\n",
      " 14  project.id                         2949 non-null   int64  \n",
      " 15  project.user_name                  2949 non-null   object \n",
      " 16  project.project_name               2949 non-null   object \n",
      " 17  project.annotation                 0 non-null      float64\n",
      " 18  project.test_name                  2949 non-null   object \n",
      " 19  project.sample_names.sample_names  2949 non-null   object \n",
      " 20  project.neutral_filler             0 non-null      float64\n",
      " 21  project.qpc20                      2949 non-null   int64  \n",
      " 22  project.qpc50                      2949 non-null   int64  \n",
      " 23  project.qpc80                      2949 non-null   int64  \n",
      " 24  project.qpc100                     2949 non-null   int64  \n",
      " 25  project.notes                      0 non-null      float64\n",
      " 26  test_id                            2949 non-null   object \n",
      "dtypes: bool(1), float64(4), int64(8), object(14)\n",
      "memory usage: 602.0+ KB\n"
     ]
    }
   ],
   "source": [
    "initial_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Counting unique values to identify classes](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sample_name\n",
       "vitamin-c              636\n",
       "paracetamol            326\n",
       "benzyl-penicillin      317\n",
       "penicillin-procaine    310\n",
       "quinine                298\n",
       "amoxicillin            296\n",
       "lactose                225\n",
       "cellulose              215\n",
       "starch                 214\n",
       "paracetamol-starch     112\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = initial_data.sample_name.nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "display(initial_data.sample_name.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Check for deleted samples](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted cards: 0\n"
     ]
    }
   ],
   "source": [
    "# Filter by deleted\n",
    "num_cards = len(initial_data.index)\n",
    "initial_data = initial_data[~initial_data['deleted']]\n",
    "\n",
    "print(f\"Deleted cards: {num_cards - len(initial_data.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Add `URL` colum to the data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add url to dataframe\n",
    "initial_data['url'] = initial_data['processed_file_location'].apply(lambda x: f\"https://pad.crc.nd.edu/{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Add column `image_name`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data['image_name'] = initial_data.apply(lambda x: create_filename(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Show number of samples by `sample_name`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vitamin-c</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paracetamol</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>benzyl-penicillin</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penicillin-procaine</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quinine</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>amoxicillin</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lactose</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cellulose</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>starch</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paracetamol-starch</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample_name  counts\n",
       "0            vitamin-c     636\n",
       "1          paracetamol     326\n",
       "2    benzyl-penicillin     317\n",
       "3  penicillin-procaine     310\n",
       "4              quinine     298\n",
       "5          amoxicillin     296\n",
       "6              lactose     225\n",
       "7            cellulose     215\n",
       "8               starch     214\n",
       "9   paracetamol-starch     112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_data.value_counts(['sample_name']).reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Exclude samples with issues](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with issues: 0 samples\n"
     ]
    }
   ],
   "source": [
    "# select cards that have no issues\n",
    "size_before = len(initial_data.index)\n",
    "initial_data = initial_data[initial_data['issue'].isnull()].copy()\n",
    "size_after = len(initial_data.index)\n",
    "\n",
    "print(f\"Samples with issues: {size_before-size_after} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Find and check samples with `sample_name`==`unknown` or empty](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2949\n",
      "Total num of samples with empty sample_name: 0 samples\n",
      "Total num of samples with unknown sample_name: 0 samples\n"
     ]
    }
   ],
   "source": [
    "column_name = \"sample_name\"\n",
    "\n",
    "print(f\"Total samples: {len(initial_data.index)}\")\n",
    "\n",
    "empty_name = filter_by_empty_column(initial_data, column_name)\n",
    "print(f\"Total num of samples with empty {column_name}: {len(empty_name.index)} samples\")\n",
    "\n",
    "unknown_name = filter_by_unknown_column(initial_data, column_name)\n",
    "print(f\"Total num of samples with unknown sample_name: {len(unknown_name.index)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Checking if there are any missing and unknown values in the `processed_file_location` column](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of samples with empty 'processed_file_location': 0 samples\n",
      "Total num of samples with unknown 'processed_file_location': 0 samples\n"
     ]
    }
   ],
   "source": [
    "column_name = \"processed_file_location\"\n",
    "\n",
    "empty_name = filter_by_empty_column(initial_data, column_name)\n",
    "print(f\"Total num of samples with empty '{column_name}': {len(empty_name.index)} samples\")\n",
    "\n",
    "unknown_name = filter_by_unknown_column(initial_data, column_name)\n",
    "print(f\"Total num of samples with unknown '{column_name}': {len(unknown_name.index)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Ckeck the urls for the `processed_file_location` column](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_name = \"url\"\n",
    "\n",
    "# bad_urls_df = check_url(initial_data)\n",
    "# print(f\"Samples with bad urls: {len(bad_urls_df.index)} samples\")\n",
    "\n",
    "# # save the samples that have a status code different from 200 in a new csv file called check_samples_with_bad_urls.csv\n",
    "# if len(bad_urls_df.index) > 0:\n",
    "#     bad_urls_df.to_csv('../data/intermediate/FHI2020_analysis/check_samples_with_bad_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Hash](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Downloading images and Calculating the hash of the processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_status_code(url):\n",
    "    r = requests.head(url, verify=False)\n",
    "    return r.status_code\n",
    "\n",
    "def get_hash(sample, folder):\n",
    "    filename = create_filename(sample)\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    #print(sample['id'], sample['url_status_code'], filepath)\n",
    "    if sample['url_status_code'] == 200:\n",
    "        if os.path.isfile(filepath):\n",
    "            hashlib_md5 = hashlib.md5(open(filepath,'rb').read()).hexdigest()\n",
    "            #print(hashlib_md5)\n",
    "            return hashlib_md5\n",
    "        else:\n",
    "            print(f\"File not found {filepath}. Downloading...\")\n",
    "            status_code = save_image_from_url(sample, folder)\n",
    "            if status_code == 200:\n",
    "                hashlib_md5 = hashlib.md5(open(filepath,'rb').read()).hexdigest()\n",
    "                return hashlib_md5\n",
    "            else:\n",
    "                print(f\"Error downloading {sample['id']}. File not found {filepath}\")\n",
    "                return None\n",
    "    else:        \n",
    "        return None\n",
    "\n",
    "def save_image_from_url(sample, output_folder):\n",
    "    r = requests.get(sample.url, verify=False)\n",
    "    output_path = os.path.join(\n",
    "        output_folder, create_filename(sample)\n",
    "    )\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "    # Fix image if needed\n",
    "    img = Image.open(output_path)\n",
    "    try:\n",
    "        \n",
    "        img.save(output_path)\n",
    "        img = Image.open(output_path)\n",
    "        img.verify()\n",
    "    except Exception as exc:\n",
    "        print(f\"Image {output_path} is broken\")\n",
    "    \n",
    "    status_code = r.status_code\n",
    "    r.close()\n",
    "    return status_code\n",
    "\n",
    "def get_hash_all(df, folder):    \n",
    "    column_exists = \"url_status_code\" in df.columns\n",
    "    \n",
    "    if ~column_exists:\n",
    "        df['url_status_code'] = df['url'].apply(lambda x: get_url_status_code(x))\n",
    "    \n",
    "    hash_codes = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(get_hash, sample, folder): sample for _, sample in df.iterrows()\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(\n",
    "            future_to_url\n",
    "        ):\n",
    "            sample = future_to_url[future]\n",
    "            try:               \n",
    "                #print(sample['id'], sample['url'], sample['url_status_code'] ,future.result())\n",
    "                hash_codes.append([sample['id'], sample['url_status_code'], future.result()])\n",
    "                \n",
    "            except Exception as exc:\n",
    "                print(\n",
    "                    \"%s generated an exception: %s\" % (sample['id'], exc)\n",
    "                )\n",
    "    return hash_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_codes = get_hash_all(initial_data, TMP_IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([200])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_codes_df = pd.DataFrame(hash_codes, columns=['id', 'url_status_code', 'hashlib_md5'])\n",
    "hash_codes_df.url_status_code.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check is all images have hash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with null hash: 0\n",
      "Samples with no image: 0\n"
     ]
    }
   ],
   "source": [
    "no_hash = hash_codes_df[hash_codes_df.hashlib_md5.isnull()]\n",
    "print(f\"Samples with null hash: {len(no_hash)}\")\n",
    "\n",
    "no_image = hash_codes_df[hash_codes_df.url_status_code != 200]\n",
    "print(f\"Samples with no image: {len(no_image)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, you can create a new column called `hashlib_md5` with the hash of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sample_name', 'test_name', 'user_name', 'date_of_creation',\n",
       "       'raw_file_location', 'processed_file_location', 'processing_date',\n",
       "       'camera_type_1', 'notes', 'sample_id', 'quantity', 'deleted', 'issue',\n",
       "       'project.id', 'project.user_name', 'project.project_name',\n",
       "       'project.annotation', 'project.test_name',\n",
       "       'project.sample_names.sample_names', 'project.neutral_filler',\n",
       "       'project.qpc20', 'project.qpc50', 'project.qpc80', 'project.qpc100',\n",
       "       'project.notes', 'test_id', 'url', 'image_name', 'url_status_code_x',\n",
       "       'url_status_code_y', 'hashlib_md5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop 'url_status_code' column from dataframes\n",
    "# hash_codes_df.drop(columns=['url_status_code'], inplace=True)\n",
    "#if 'url_status_code' in df.columns: df.drop(columns=['url_status_code'], inplace=True)\n",
    "\n",
    "# merge for adding the 'hashlib_md5' column\n",
    "initial_data = pd.merge(initial_data, hash_codes_df, on='id')\n",
    "\n",
    "initial_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check if there are any samples that have the same hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Total unique hash codes : 2949\n",
      "Total of hash code with one sample: 2949\n",
      "Total of hash code with two or more samples: 0\n",
      "\n",
      "Total of samples: 2949\n",
      "Total of samples without duplicates: 2949\n",
      "Total of samples in some duplicate case (will be deleted): 0\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(initial_data)\n",
    "data = initial_data.groupby(['hashlib_md5']).size().reset_index(name='counts')\n",
    "one_sample_hash = data[data['counts']==1]\n",
    "two_more_sample_hash = data[data['counts']>1]\n",
    "\n",
    "print('Summary:')\n",
    "print(f\"Total unique hash codes : {len(data.index)}\")\n",
    "print(f\"Total of hash code with one sample: {len(one_sample_hash.index)}\")\n",
    "print(f\"Total of hash code with two or more samples: {len(two_more_sample_hash.index)}\")\n",
    "\n",
    "print('')\n",
    "print(f\"Total of samples: {num_samples}\")\n",
    "print(f\"Total of samples without duplicates: {len(data.index)}\")\n",
    "print(f\"Total of samples in some duplicate case (will be deleted): {num_samples-len(data.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sort by `sample_name` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = initial_data.sort_values(by=['sample_name', 'id', 'sample_id'], ascending=[True, False, False])\n",
    "sorted_data_reset = sorted_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Save Cleaned Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On that point the dataframe `data` should have the cleaned data samples to put in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned dataframe to csv\n",
    "data = sorted_data[['id','sample_id','sample_name', 'quantity', 'project.id', 'url', 'hashlib_md5', 'image_name']]\n",
    "data.to_csv(OUTPUT_DATA_FNAME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
