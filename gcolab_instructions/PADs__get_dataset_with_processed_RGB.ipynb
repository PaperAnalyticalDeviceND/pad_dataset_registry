{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g25AOynKHDUF"
      },
      "source": [
        "# Accessing PAD Datasets\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Welcome to our Quick Start Guide!** In this notebook, we'll guide you on how to access datasets from the [Paper Analytical Device (PAD) Dataset Registry](https://github.com/PaperAnalyticalDeviceND/pad_dataset_registry) for model training.\n",
        "\n",
        "This guide includes detailed steps on setting up your environment, installing necessary dependencies, exploring available datasets, downloading your selected dataset, storing it locally, and visualizing its metadata.\n",
        "\n",
        "**User-Friendly:** Designed with Google Colab in mind, this guide is also compatible with any environment supporting Python 3.9 or newer.\n",
        "\n",
        "If you have any questions or need further assistance, please feel free to contact pmoreira@nd.edu.\n",
        "\n",
        "Enjoy your journey through the datasets and happy modeling!\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry_o4SzpBbQh"
      },
      "source": [
        "# Setup Enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7deDOc7U7DX"
      },
      "outputs": [],
      "source": [
        "# Setup Environment\n",
        "\n",
        "# Install dependencies required for the project. This includes DVC for data version control,\n",
        "# and DVC-GDrive to enable storage integration with Google Drive for dataset storage.\n",
        "!pip install dvc dvc-gdrive &> /dev/null\n",
        "\n",
        "# The line below is commented out because these packages are already installed\n",
        "# in the Colab; uncomment if you are on your local computer\n",
        "# !pip install pandas opencv-python matplotlib\n",
        "\n",
        "# Define constants for file and directory names to be used in the project.\n",
        "DEV_FNAME = 'metadata_dev.csv'  # Name of the development dataset metadata file.\n",
        "TEST_FNAME = 'metadata_test.csv'  # Name of the test dataset metadata file.\n",
        "DEV_IMAGES_PATH = 'images_dev'  # Directory path for development dataset images.\n",
        "TEST_IMAGES_PATH = 'images_test'  # Directory path for test dataset images.\n",
        "REPORT_PATH = 'report'  # Directory path for storing reports generated from analyses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--39Rjm9zg0W"
      },
      "source": [
        "# **List** Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7z_9SHyWSB5"
      },
      "outputs": [],
      "source": [
        "# Use the `dvc list` command to retrieve a list of datasets available in the PAD Dataset Registry.\n",
        "!dvc list https://github.com/PaperAnalyticalDeviceND/pad_dataset_registry datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ePBiYmzlz6"
      },
      "source": [
        "# **Download** a dataset from the previous dataset list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyE8HxeInIGv"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9ifOQb_nMP2"
      },
      "outputs": [],
      "source": [
        "##****************************************************************************##\n",
        "# Downloading functions\n",
        "##****************************************************************************##\n",
        "import csv, os\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download_file(url, filename, images_path):\n",
        "    \"\"\"Download a file from a URL and save it to a local file.\"\"\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        path = os.path.join(images_path, filename)\n",
        "        with open(path, 'wb') as f:\n",
        "            for chunk in response.iter_content(1024):\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_files_from_csv_file(file_path, images_path):\n",
        "    \"\"\"Download files in parallel based on URLs from a CSV file with a progress bar.\"\"\"\n",
        "    # Open the CSV file and parse its content\n",
        "    with open(file_path, newline='') as csvfile:\n",
        "        rows = list(csv.DictReader(csvfile)) # Convert to list for tqdm\n",
        "\n",
        "        # Initialize tqdm for the progress bar\n",
        "        pbar = tqdm(total=len(rows), desc=\"Downloading files\")\n",
        "\n",
        "        def update(*args):\n",
        "            # Update the progress bar by one each time a file is downloaded\n",
        "            pbar.update()\n",
        "\n",
        "        # Use ThreadPoolExecutor to download files in parallel\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            futures = []\n",
        "            for row in rows:\n",
        "                url = row['url']\n",
        "                filename = row['image_name']\n",
        "                # Schedule the download task\n",
        "                future = executor.submit(download_file, url, filename, images_path)\n",
        "                future.add_done_callback(update)\n",
        "                futures.append(future)\n",
        "\n",
        "            # Wait for all futures to complete\n",
        "            for future in futures:\n",
        "                future.result()\n",
        "\n",
        "        # Close the progress bar\n",
        "        pbar.close()\n",
        "\n",
        "\n",
        "##****************************************************************************##\n",
        "# Preprocessing functions: Extract RGB Information for FHI360\n",
        "##****************************************************************************##\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import warnings\n",
        "import cv2 as cv\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from datetime import datetime\n",
        "import ssl\n",
        "\n",
        "SAVE_DIR = './pixel_data/'\n",
        "REQS = {'LOG':'log.txt'}\n",
        "\n",
        "HORIZONTAL_BORDER = 12\n",
        "VERTICAL_BORDER = 0\n",
        "\n",
        "BLACK_THRESH_S = 35\n",
        "BLACK_THRESH_V = 70\n",
        "\n",
        "!touch temp.png # Creates a temporary file for image processing\n",
        "\n",
        "#Takes a list of pixels and a BGR image and returns the average\n",
        "# Lab pixel values\n",
        "def px_avgPixelsLAB(pixels, img):\n",
        "  \"\"\"Calculate the average Lab pixel values for a list of pixels in a BGR image.\"\"\"\n",
        "  workingImg = cv.cvtColor(img, cv.COLOR_BGR2Lab)\n",
        "  totalL = 0\n",
        "  totalA = 0\n",
        "  totalB = 0\n",
        "  for pixel in pixels:\n",
        "    x = pixel[0]\n",
        "    y = pixel[1]\n",
        "    l, a, b = workingImg[x,y,:]\n",
        "    totalL += l\n",
        "    totalA += a\n",
        "    totalB += b\n",
        "  if len(pixels) != 0:\n",
        "    totalL /= len(pixels)\n",
        "    totalA /= len(pixels)\n",
        "    totalB /= len(pixels)\n",
        "  return int(totalL + 0.5), int(totalA + 0.5), int(totalB + 0.5)\n",
        "\n",
        "\n",
        "#Takes a list of pixels and a BGR image and returns the average\n",
        "# RGB pixel values\n",
        "def px_avgPixels(pixels, img):\n",
        "  \"\"\"Calculate the average RGB pixel values for a list of pixels in a BGR image.\"\"\"\n",
        "  totalB = 0\n",
        "  totalG = 0\n",
        "  totalR = 0\n",
        "  for pixel in pixels:\n",
        "    x = pixel[0]\n",
        "    y = pixel[1]\n",
        "    b,g,r = img[x,y,:]\n",
        "    totalB += b\n",
        "    totalG += g\n",
        "    totalR += r\n",
        "  if len(pixels) != 0:\n",
        "    totalB /= len(pixels)\n",
        "    totalG /= len(pixels)\n",
        "    totalR /= len(pixels)\n",
        "  return int(totalR + 0.5), int(totalG + 0.5), int(totalB + 0.5)\n",
        "\n",
        "\n",
        "#Takes a distance from a center and returns a weight between 0 and 1\n",
        "# determined by cosine such that a point at the cetner has weight 1,\n",
        "# and a point at the extremes has weight ~0.\n",
        "def intFind_cosCorrectFactor(dx, dy, centerX, centerY):\n",
        "  \"\"\"Determine a weight between 0 and 1 based on distance from center, using a cosine function.\"\"\"\n",
        "  relevantD = max((dx/centerX), dy/centerY)\n",
        "  relevnatDRads = (math.pi/2) * relevantD\n",
        "  return math.cos(relevnatDRads)\n",
        "\n",
        "#Takes a HSV image and returns a list of the most intense pixels in it,\n",
        "# after applying filtering to minimize black bars on the edges\n",
        "def intFind_findMaxIntensitiesFiltered(img):\n",
        "  \"\"\"Return a list of pixels with maximum intensity, filtering out black bars on edges.\"\"\"\n",
        "  imgS = img[:,:,1]\n",
        "  imgV = img[:,:,2]\n",
        "  maxI = 0\n",
        "  maxSet = []\n",
        "  centerX = imgS.shape[0]/2\n",
        "  centerY = imgS.shape[1]/2\n",
        "\n",
        "  for i in range(imgS.shape[0]):\n",
        "    dX = abs(centerX-i)\n",
        "    for j in range(imgS.shape[1]):\n",
        "      dY = abs(centerY-j)\n",
        "      sF = intFind_cosCorrectFactor(dX,dY,centerX,centerY)\n",
        "      cS = sF*imgS[i,j]\n",
        "      cV = sF*imgV[i,j]\n",
        "      if cS <= BLACK_THRESH_S and cV <= BLACK_THRESH_V:\n",
        "        pass\n",
        "      else:\n",
        "        maxSet.append((i,j))\n",
        "  return maxSet\n",
        "\n",
        "def fm_genIndex(regions, ColorList = ['R', 'G', 'B']):\n",
        "  \"\"\"Generate an index for CSV column headers based on the number of regions and color list.\"\"\"\n",
        "\n",
        "  index = ['Image', 'Contains', 'Drug %', 'PAD S#']\n",
        "  for letter in ['A','B','C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']:\n",
        "    for j in range(1,regions+1):\n",
        "      for color in ColorList:\n",
        "        tempStr = letter+str(j)+'-'+color\n",
        "        index.append(tempStr)\n",
        "  return index\n",
        "\n",
        "def fm_checkFormating(dir=SAVE_DIR, errorsFile=None):\n",
        "  \"\"\"Check formatting and required files in the specified directory.\"\"\"\n",
        "  if not os.path.isdir(dir):\n",
        "    os.mkdir(dir)\n",
        "  if errorsFile is None:\n",
        "    errors = open(dir+REQS['LOG'], 'a')\n",
        "  else:\n",
        "    errors = errorsFile\n",
        "  files = os.listdir(dir)\n",
        "  #print(files)\n",
        "  for item in REQS.values():\n",
        "    if item not in files:\n",
        "      errorString = str.format(\"Required file %s not found, creating.\\n\" %(item))\n",
        "      errors.write(errorString)\n",
        "      warnings.warn(errorString)\n",
        "      if item is REQS['LOG']:\n",
        "        temp = open(dir+item, 'w')\n",
        "        temp.close()\n",
        "      else:\n",
        "        os.mkdir(dir+item)\n",
        "  if errorsFile is None:\n",
        "    errors.close()\n",
        "\n",
        "def _regionGen(regions, region):\n",
        "  \"\"\"Generate start and end points for a given region.\"\"\"\n",
        "  start = 359\n",
        "  totalLength = 273\n",
        "  regionStart = start + math.floor(totalLength * (region/regions)) + VERTICAL_BORDER\n",
        "  regionEnd = start + math.floor(totalLength * ((region+1)/regions)) - VERTICAL_BORDER\n",
        "  return regionStart, regionEnd\n",
        "\n",
        "def _fullRoutine(img, roiFunc, df, RGB=True, regions=3):\n",
        "  \"\"\"Complete routine for processing an image and extracting pixel information.\"\"\"\n",
        "\n",
        "  letters = ['A','B','C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
        "  rList = []\n",
        "  gList = []\n",
        "  bList = []\n",
        "  imgHSV = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
        "  for lane in range(1,13):\n",
        "    laneStart = 17 + (53*lane)+HORIZONTAL_BORDER\n",
        "    laneEnd = 17 + (53*(lane+1))-HORIZONTAL_BORDER\n",
        "    letter = letters[lane-1]\n",
        "    for region in range(regions):\n",
        "      regionStart, regionEnd = _regionGen(regions, region)\n",
        "      roi = imgHSV[regionStart:regionEnd,laneStart:laneEnd,:]\n",
        "      rgbROI = img[regionStart:regionEnd,laneStart:laneEnd,:]\n",
        "      pixels = roiFunc(roi)\n",
        "      tempString = letter + str(region+1) + \"-\"\n",
        "      #Switches between RGB and Lab\n",
        "      if(RGB):\n",
        "        r, g, b = px_avgPixels(pixels, rgbROI)\n",
        "        df[tempString+'R'] = r\n",
        "        df[tempString+'G'] = g\n",
        "        df[tempString+'B'] = b\n",
        "      else:\n",
        "        l, a, blu = px_avgPixelsLAB(pixels, rgbROI)\n",
        "        df[tempString+'L'] = l\n",
        "        df[tempString+'a'] = a\n",
        "        df[tempString+'b'] = blu\n",
        "  return df\n",
        "\n",
        "\n",
        "def addIndex(runSettings):\n",
        "  \"\"\"Add index information to run settings based on regions and color mode (RGB or Lab).\"\"\"\n",
        "  for setting in runSettings:\n",
        "    regions = runSettings[setting]['regions']\n",
        "    if(runSettings[setting]['RGB']):\n",
        "      runSettings[setting]['Index'] = fm_genIndex(regions)\n",
        "    else:\n",
        "      runSettings[setting]['Index'] = fm_genIndex(regions, ['L','a','b'])\n",
        "  return runSettings\n",
        "\n",
        "def regionRoutine(target, runSettings, save_dir=SAVE_DIR):\n",
        "  \"\"\"Read a CSV file and process images according to the run settings.\"\"\"\n",
        "\n",
        "  ssl._create_default_https_context = ssl._create_unverified_context\n",
        "  \n",
        "  startTime = datetime.now()\n",
        "  url = 'https://pad.crc.nd.edu'\n",
        "  dest = 'temp.png'\n",
        "  fm_checkFormating(save_dir)\n",
        "  errors = open(save_dir+REQS['LOG'], 'a')\n",
        "  print(\"Starting...\")\n",
        "  with open(target) as csvfile:\n",
        "    csvreader = csv.reader(csvfile, )\n",
        "    i = 0\n",
        "    next(csvreader)  # Skip the first line\n",
        "    for row in csvreader:\n",
        "      cTime = datetime.now()\n",
        "      i+=1\n",
        "      try:\n",
        "        urllib.request.urlretrieve(row[5], dest)\n",
        "        #urllib.request.urlretrieve(row[7], dest)\n",
        "        img = cv.imread(dest)\n",
        "        if (1250, 730, 3) != img.shape and (1220, 730, 3) != img.shape:\n",
        "          errorString = str.format(\"Error with file %s. Expected shape %s, found shape %s.\\n\" %(file, '(1250, 730, 3) or (1220, 730, 3)', str(img.shape)))\n",
        "          errors.write(errorString)\n",
        "          warnings.warn(errorString)\n",
        "        else:\n",
        "          for setting in runSettings:\n",
        "            data = {}\n",
        "            data = _fullRoutine(img, intFind_findMaxIntensitiesFiltered, data, runSettings[setting]['RGB'], runSettings[setting]['regions'])\n",
        "            #data['Image'] = row[0]\n",
        "            #data['Contains'] = row[1]\n",
        "            #data['Drug %'] = row[18]\n",
        "            #data['PAD S#'] = row[17]\n",
        "\n",
        "            data['Image'] = row[0]\n",
        "            data['Contains'] = row[2]\n",
        "            data['Drug %'] = row[3]\n",
        "            data['PAD S#'] = row[1]\n",
        "\n",
        "\n",
        "            df = pd.DataFrame(data, columns=runSettings[setting]['Index'], index=[data['Image']])\n",
        "            if(not os.path.exists(save_dir+setting)):\n",
        "              df.to_csv(save_dir+setting, mode='w', header=True)\n",
        "            else:\n",
        "              df.to_csv(save_dir+setting, mode='a', header=False)\n",
        "          elapsedTime = datetime.now() - cTime\n",
        "          print(\"Finished image \",row[0],\" in \",elapsedTime)\n",
        "      except Exception as e:\n",
        "        errorString = str.format(\"Error %s with file %s.\\n\" %(str(e), row[0]))\n",
        "        errors.write(errorString)\n",
        "        warnings.warn(errorString)\n",
        "      os.remove(dest)\n",
        "    errors.close()\n",
        "    endTime = datetime.now()\n",
        "    regions = 3+12+20\n",
        "    print('Time: ',endTime-startTime, ' time saved = ',i*regions*13/60.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvi8wSgJnnAN"
      },
      "source": [
        "## Define parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pkb_e3lyqiJ"
      },
      "outputs": [],
      "source": [
        "## Parameters\n",
        "# Specify the name of the dataset to use. This should match one of the datasets listed in the previous step.\n",
        "dataset_name = 'FHI2020_Stratified_Sampling'\n",
        "\n",
        "# Toggle whether to download files from the Development (DEV) set.\n",
        "download_dev = True\n",
        "\n",
        "# Toggle whether to download files from the Test (TEST) set.\n",
        "download_test = False\n",
        "\n",
        "# Decide if original images from the cards should be downloaded.\n",
        "# This is useful if detailed analysis of the original images is required.\n",
        "download_original_images = False\n",
        "\n",
        "# Enable downloading and preprocessing of pixel data to extract RGB information,\n",
        "# which is essential for certain types of analysis, such as FHI360.\n",
        "download_pixel_data = True\n",
        "\n",
        "# Define the number of regions to segment the image into during preprocessing.\n",
        "# This is only required if `download_pixel_data` is set to True.\n",
        "num_regions = 10\n",
        "\n",
        "## Initial Setup\n",
        "\n",
        "# Create a directory to store all dataset files.\n",
        "# If the directory already exists, it will not throw an error due to `exist_ok=True`.\n",
        "os.makedirs(dataset_name, exist_ok=True)\n",
        "\n",
        "# Specify the path to save the processed pixel data.\n",
        "# This organizes the output from preprocessing, facilitating easier analysis.\n",
        "pixel_data_path = os.path.join(dataset_name, \"pixel_data/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHYC7DWZq_65"
      },
      "source": [
        "## Get DEV set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5GPXXFRa9bo"
      },
      "outputs": [],
      "source": [
        "# **** DEV ****\n",
        "\n",
        "# Proceed if downloading of the development dataset is enabled\n",
        "if download_dev:\n",
        "    # Download the DEV metadata file using DVC. `--force` ensures the latest version is downloaded.\n",
        "    !dvc get --force https://github.com/PaperAnalyticalDeviceND/pad_dataset_registry datasets/$dataset_name/$DEV_FNAME -o $dataset_name/$DEV_FNAME\n",
        "\n",
        "    # Define the path to save the dev metadata file within the dataset directory\n",
        "    dev_metadata_path = os.path.join(dataset_name, DEV_FNAME)\n",
        "\n",
        "    # Check if original images from the DEV set should be downloaded\n",
        "    if download_original_images:\n",
        "        # Specify the folder within the dataset directory to save the images\n",
        "        images_path = os.path.join(dataset_name, DEV_IMAGES_PATH)\n",
        "\n",
        "        # Ensure the directory exists; creates it if it does not\n",
        "        os.makedirs(images_path, exist_ok=True)\n",
        "\n",
        "        # Initiate the download of image files for the dev set based on the metadata\n",
        "        download_files_from_csv_file(dev_metadata_path, images_path)\n",
        "\n",
        "    # Check if pixel data should be processed and extracted for RGB information analysis\n",
        "    if download_pixel_data:\n",
        "        # Name of the output file where the RGB data will be saved\n",
        "        output_fname = 'rgb__dev.csv'\n",
        "\n",
        "        # The input file (metadata path) to process for RGB data extraction\n",
        "        input_fname = dev_metadata_path\n",
        "\n",
        "        # Define settings for the RGB data extraction process, including number of regions\n",
        "        runs = {f\"{num_regions}_region_{output_fname}\": {'RGB': True, 'regions': num_regions}}\n",
        "\n",
        "        # Process the specified input file and save the RGB data to the defined output directory\n",
        "        regionRoutine(input_fname, addIndex(runs), save_dir=pixel_data_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5LwtoldrNZD"
      },
      "source": [
        "## Get TEST set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymdbmQl_rQ7T"
      },
      "outputs": [],
      "source": [
        "# **** TEST ****\n",
        "\n",
        "# Proceed if downloading of the test dataset is enabled\n",
        "if download_test:\n",
        "    # Download the TEST metadata file using DVC. The command fetches the latest version of the file.\n",
        "    !dvc get https://github.com/PaperAnalyticalDeviceND/pad_dataset_registry datasets/$dataset_name/$TEST_FNAME -o $dataset_name/$TEST_FNAME\n",
        "\n",
        "    # Define the path to save the test metadata file within the dataset directory\n",
        "    test_metadata_path = os.path.join(dataset_name, TEST_FNAME)\n",
        "\n",
        "    # Check if original images from the TEST set should be downloaded\n",
        "    if download_original_images:\n",
        "        # Specify the folder within the dataset directory to save the images\n",
        "        images_path = os.path.join(dataset_name, TEST_IMAGES_PATH)\n",
        "\n",
        "        # Ensure the directory exists; creates it if it does not\n",
        "        os.makedirs(images_path, exist_ok=True)\n",
        "\n",
        "        # Initiate the download of image files for the test set based on the metadata\n",
        "        download_files_from_csv_file(test_metadata_path, images_path)\n",
        "\n",
        "    # Check if pixel data should be processed and extracted for RGB information analysis\n",
        "    if download_pixel_data:\n",
        "        # Name of the output file where the RGB data will be saved\n",
        "        output_fname = 'rgb__test.csv'\n",
        "\n",
        "        # The input file (metadata path) to process for RGB data extraction\n",
        "        input_fname = test_metadata_path\n",
        "\n",
        "        # Define settings for the RGB data extraction process, including the number of regions\n",
        "        runs = {f\"{num_regions}_region_{output_fname}\": {'RGB': True, 'regions': num_regions}}\n",
        "\n",
        "        # Process the specified input file and save the RGB data to the defined output directory\n",
        "        regionRoutine(input_fname, addIndex(runs), save_dir=pixel_data_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql_oHKpQ3mBc"
      },
      "source": [
        "# **Save** the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSdX-FS83rm2"
      },
      "source": [
        "## **Save Data to Google Drive (Recommended)**\n",
        "\n",
        "To ensure the safety and accessibility of your dataset, we recommend saving it directly to a folder in your Google Drive. This section guides you through the process of mounting your Google Drive in this environment and copying the dataset folder to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wkUkD3FcpQH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access its file system.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path in Google Drive where you want to save the dataset.\n",
        "my_path = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# Copy the entire dataset directory to the specified path in Google Drive.\n",
        "!cp -r $dataset_name/ $my_path\n",
        "\n",
        "# Confirm the dataset has been copied and provide a direct link to Google Drive.\n",
        "print(f\"\\nNow you can find the Dataset folder named `{dataset_name}` in your Google Drive.\")\n",
        "print(\"For quick access to your Google Drive, visit: https://drive.google.com/drive/u/0/my-drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtYZtsLG31cj"
      },
      "source": [
        "> ## Or save it on your computer (slow)\n",
        "\n",
        "\n",
        "\n",
        "> Uncomment the lines so you can save the dataset in your computer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQAX71F733c3"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# !zip -r $dataset_name.zip $dataset_name/ &> /dev/null\n",
        "# files.download(f\"{dataset_name}.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGZIdgS4Likh"
      },
      "source": [
        "# Visualize the metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9jLhU_IK7vJ"
      },
      "outputs": [],
      "source": [
        "# Visualize the metadata using pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(dev_metadata_path)s\n",
        "\n",
        "data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ry_o4SzpBbQh",
        "pyE8HxeInIGv",
        "M5LwtoldrNZD",
        "mtYZtsLG31cj"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
